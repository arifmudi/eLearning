{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"black\">https://bit.ly/wfh-2021-taudata</font></center>\n",
    "    \n",
    "<img alt=\"\" src=\"images/IDBigData/Cover_idBigData-2021.jpg\" /> \n",
    "    \n",
    "## <center><font color=\"blue\">tau-data Indonesia</font><br>(C) Taufik Sutanto - 2021</center>\n",
    "<center><a href=\"https://tau-data.id\">https://tau-data.id</a> ~ <a href=\"mailto:taufik@tau-data.id\">taufik@tau-data.id</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Tak Kenal, Maka Tak Sayang</font></center>\n",
    "\n",
    "<center><img src=\"images/bio-about/bio_TS.png\" /></center>\n",
    "\n",
    "<font color=\"green\">“*I am not what you see. I am what time and effort and interaction slowly unveil*.” ― Richelle E. Goodrich</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">WFH 2021: Pengenalan Natural Language Processing (NLP) ~ Sentiment Analysis</font></center>\n",
    "\n",
    "* Pendahuluan NLP & Textmining\n",
    "* **Basic Document Representation**\n",
    " - Vector Space Model: tf-idf ~ BM25\n",
    " - normalization, Stopwords, & n-gram\n",
    " - *Discussion*: on Strength & weakness of VSM and-or tf-idf\n",
    "* **Simple Text Preprocessing**\n",
    " - lemma, Slang & Abbreviation\n",
    "* **Unsupervised Sentiment Analysis: Lexicon Based**\n",
    " - *Discussion*: Strength & weakness, Error & Model Analysis + Interpretation\n",
    "* **Supervised Approach VSM + SVM**\n",
    " - *Discussion*: Strength & weakness, Error & Model Analysis + Interpretation\n",
    "* **Sentimen Analysis via Deep Learning**\n",
    " - Word Embedding: Word2Vec & FastText\n",
    " - LSTM for Sentiment Analysis\n",
    " - *Discussion*: Strength & weakness, Error & Model Analysis + Interpretation\n",
    "* **Penutup**\n",
    " - Further works, Recent approaches & problems in Sentiment Analysis\n",
    "\n",
    "<font color=\"green\">\"*I always have a basic plot outline, but I like to leave some things to be decided while I write*.\" ~ J. K. Rowling</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Asumsi Workshop WFH 2021 - IDBigData</font></center>\n",
    "\n",
    "* <font color=\"green\">**Tantangan**</font>: \n",
    " - Peserta dari berbagai latar belakang keilmuan dan tingkat kemampuan.\n",
    " - Hanya 3 jam (termasuk diskusi & rehat)\n",
    " - Daring (online)\n",
    "* <font color=\"green\">**Asumsi & Strategi**</font>:\n",
    " - Peserta diasumsikan mengenal sedikit tentang Python.\n",
    " - Hanya menggunakan kasus teks bahasa Indonesia\n",
    " - Fokus ke **Aplikasi Dasar** NLP/Textmining Sentimen Analisis.\n",
    " - **Code** diberikan/tunjukkan hanya sebagai demonstrasi (bukan kompetensi utama yang didapatkan dari kegiatan ini).\n",
    "\n",
    "<center><img src=\"images/vector-icon/quiz-requirement.jpeg\" width=\"399\" height=\"249\" /></center>\n",
    "\n",
    "><font color=\"green\">\"*Minimum requirements & Maximum adjustments are two steps for Happy & Successful Life*\" ~ Maulik Jadav</font>\n",
    "\n",
    "image source: https://medium.com/@nipunithisarangi/level-up-your-requirement-gathering-game-cb1c42e9ffbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Referensi & Resources:</font></center>\n",
    "\n",
    "### <font color=\"green\">Python:</font>\n",
    "* Pengenalan Python: https://tau-data.id/adsp/ & https://tau-data.id/hpds/\n",
    "* Python basic: https://www.python-course.eu/python3_history_and_philosophy.php \n",
    "* Data Science Basic: https://tau-data.id/dsbd/ & https://scikit-learn.org/stable/tutorial/index.html\n",
    "* Advanced Python: http://andy.terrel.us/blog/2012/09/27/starting-with-python/\n",
    "* Visualisasi di Python: https://matplotlib.org/gallery.html\n",
    "\n",
    "<img alt=\"\" src=\"images/tau-data_banner_large.jpg\" style=\"width: 600px;\" />\n",
    "\n",
    "### <font color=\"green\">NLP~Text Mining:</font>\n",
    "* https://tau-data.id/nlptm/ , https://tau-data.id/sma/ https://tau-data.id/sna/\n",
    "* https://github.com/ailabtelkom/id-NLP-resources\n",
    "* https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia\n",
    "* https://github.com/fajri91/InSet\n",
    "* https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Instruksi Workshop:</font></center>\n",
    "\n",
    "* **Side-by-side**: Atur jendela antar muka layar berdampingan.\n",
    "* **Expand Sections**: Di Google Colab, klik \"View\", lalu pilih \"Expand Sections\"\n",
    "* **Shortcuts**: Selama Workshop berlangsung, tekan tombol \"ctrl+Enter\" untuk menjalankan cell.\n",
    "* **Run Anyway**: Saat pertama kali menjalankan cell akan ada warning box pop-up, silahkan pilih \"Run Anyway\"\n",
    "* **Left Panel**: Disebelah kiri Google Colab, silahkan tekan \"icon directory\", di panel ini kita dapat unduh atau unggah file/folder.\n",
    "* **Terurut (Sequential)**: Semua cell harus dijalankan terurut dari atas ke bawah, tanpa ada cell yang terlewati.\n",
    "\n",
    "<center><img src=\"images/side-by-side.jpg\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\n"
     ]
    }
   ],
   "source": [
    "import nltk, warnings; warnings.simplefilter('ignore')\n",
    "import logging; logging.captureWarnings(True)\n",
    "# Pilih \"Run Anyway\" di Pop-Up dialog yang muncul jika cell ini dijalankan di Google Colab\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.dic\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/s-pos.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/s-neg.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/s-negasi.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/ind_SA.csv\n",
    "    !pip install python-crfsuite unidecode textblob sklearn-pycrfsuite sastrawi gensim\n",
    "    nltk.download('popular')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Modules untuk Notebook ini\n",
    "import re, matplotlib.pyplot as plt, pandas as pd, seaborn as sns, json\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "random_state = 99\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Apakah Perbedaan antara NLP dan Text Mining (TM)?</font></center>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/1_jarvis.jpg\" style=\"height: 450px; width: 600px;\" /></p>\n",
    "\n",
    "<p><a href=\"https://www.turn-on.de/lifestyle/topliste/zehn-film-gadgets-die-wir-uns-im-wahren-leben-wuenschen-4413\" target=\"_blank\"><strong>[Image Source]: https://www.turn-on.de/primetime/topliste/zehn-film-gadgets-die-wir-uns-im-wahren-leben-wuenschen-4413</strong></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">NLP dan Text Mining</font></center>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/1_Text_Analytics.jpg\" style=\"height: 451px; width: 600px;\" /></p>\n",
    "\n",
    "<p>[Image Source: <a href=\"http://www.pearson.com.au/products/S-Z-Turban-Sharda/Business-Intelligence-and-Analytics-Systems-for-Decision-Support-Global-Edition/9781292009209?R=9781292009209\" target=\"_blank\">Efraim T. &quot;Business Intelligence and Analytics: Systems for Decision Support, Global Edition (10e)</a>&quot;]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Aplikasi NLP dan Text Mining</font></center>\n",
    "\n",
    "* Sentiment Analysis\n",
    "* Speech Recognition dan Classification\n",
    "* Machine Translation (Misal&nbsp;https://translate.google.com/ )\n",
    "* Information Retrieval (IR)&nbsp;(misal www.google.com, bing, elasticsearch, etc.)\n",
    "* Man-Machine Interface (misal Chatbot, Siri, cortana, atau Alexa)\n",
    "* Named Entity Recognition (NER)\n",
    "* Word Sense Disambiguation\n",
    "* Topic Modelling, dsb\n",
    "\n",
    "<p><img alt=\"\" src=\"images/nlp-textmining-applications.jpg\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Sentimen Analysis</font></center>\n",
    "\n",
    "<p><strong>Apakah sentiment analysis?</strong></p>\n",
    "\n",
    "* Sentiment analysis, also called opinion mining, is the field of study that analyzes people’s opinions, sentiments, appraisals, attitudes, and emotions toward entities and their attributes expressed in written text [Bing Liu 2014].\n",
    "\n",
    "<p>Terkadang disebut juga sebagai&nbsp;<strong>opinion mining.</strong> (walau technically sebenarnya berbeda)</p>\n",
    "\n",
    "For proper definition see:\n",
    "* Liu, B., 2015. Sentiment analysis: Mining opinions, sentiments, and emotions. Cambridge University Press.\n",
    "\n",
    "<p><img style=\"undefined: undefined;\" src=\"images/sentiment-analysis.jpg\" width=\"589\" height=\"231\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Aplikasi Sentiment analysis</font></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/sentimen_analysis_interface.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_SA_techniques.jpg\" style=\"height:300px; width:536px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Preprocessing</font></center>\n",
    "\n",
    "* Data Media Sosial sangat \"noisy\".\n",
    "* Sama seperti data terstruktur: **Garbage-in~Garbage-out**\n",
    "* Sebagian besar waktu akan habis untuk preprocessing.\n",
    "* Di workshop ini hanya disinggung sekilas sebagian proses preprocessing data teks. Silahkan akses resources di cell sebelum ini untuk detail lebih jauh.\n",
    "\n",
    "<img alt=\"\" src=\"images/Data-preprocessing-steps-for-extractive-text-summarization.png\" style=\"height:400px; width:487px\" />\n",
    "\n",
    "[<a href=\"https://www.researchgate.net/publication/317610956_Evaluation_of_Unsupervised_Learning_based_Extractive_Text_Summarization_Technique_for_Large_Scale_Review_and_Feedback_Data/figures?lo=1&utm_source=google&utm_medium=organic\" target=\"_blank\"><strong>Image Source</strong></a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Text Case Normalization (Huruf BESAR/kecil)</font></center>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya (*) kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
    "\t<li><em>Text case normalization</em> dapat dilakukan pada string secara efisien.</li>\n",
    "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. Mengapa dan apa contohnya?</li>\n",
    "</ul>\n",
    "\n",
    "<p>(*) Coba temukan minimal 2 pengecualian dimana&nbsp; huruf kapital/kecil (case) mempengaruhi makna/pemrosesan teks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permisi mas, kalau mau ikut workshop wfh-2021 daftarnya kemana ya? ...@@... \n",
      "PERMISI MAS, KALAU MAU IKUT WORKSHOP WFH-2021 DAFTARNYA KEMANA YA? ...@@... \n"
     ]
    }
   ],
   "source": [
    "# Ignore case (huruf besar/kecil)\n",
    "T = \"Permisi Mas, kalau mau ikut workshop WFH-2021 daftarnya kemana ya? ...@@... \"\n",
    "print(T.lower())\n",
    "print(T.upper())\n",
    "# Perintah ini sangat efisien karena hanya merubah satu bit di setiap (awal) bytes dari setiap karakter\n",
    "# Sehingga tetap efisien jika ingin dilakukan sebelum tokenisasi dan-atau pada data yang besar sekalipun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Tokenisasi</font></center>\n",
    "\n",
    "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
    "\n",
    "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
    "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
    "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
    "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]: https://www.softwareadvice.com/resources/what-is-text-analytics/</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Tokenisasi Berbagai Bahasa</font></center>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
    "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contoh Tokenisasi dengan Module <font color=\"blue\"> TextBlob</font>\n",
    "\n",
    "<p>*Blob : Binary large Object</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Permisi', 'Mas', 'kalau', 'mau', 'ikut', 'workshop', 'WFH-2021', 'daftarnya', 'kemana', 'ya']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing di TextBlob\n",
    "T = \"Permisi Mas, kalau mau ikut workshop WFH-2021 daftarnya kemana ya? ...@@... \"\n",
    "print(TextBlob(T).words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">NLP Bahasa Indonesia</font></center>\n",
    "\n",
    "<p>NLTK belum support Bahasa Indonesia, bahkan module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
    "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
    "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
    "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
    "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
    "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
    "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
    "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Morphological-Linguistic Normalization: Stemming & Lemmatization</font></center>\n",
    "\n",
    "### (Canonical Representation)\n",
    "<img alt=\"\" src=\"images/meme-cartoon/2_yoda.jpg\" style=\"height:400px; width:400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Stemming dan Lemma</font>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
    "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
    "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
    "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1 raya itu bareng dengan saat kita pergi ke depok\n",
      "test2 raya pergi suara\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer dengan Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "print(stemmer.stem(\"Test1: perayaan itu Berbarengan dengan saat kita bepergian ke Depok\"))\n",
    "print(stemmer.stem(\"Test2: Perayaan, Bepergian, Menyuarakan\"))\n",
    "# Amati dan analisa hasilnya dengan seksama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Text Level Normalization: StopWords</font></center>\n",
    "\n",
    "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
    "<strong>Contoh</strong>:<br />\n",
    "\n",
    "<ul>\n",
    "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
    "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
    "\t<li>Stopwords twitter: RT, ...<br />\n",
    "\t<img alt=\"\" src=\"images/2_StopWords.png\" style=\"height:250px; width:419px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua']\n",
      "126 179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "stop_en = stopwords.words('english')\n",
    "stop_id = StopWordRemoverFactory().get_stop_words()\n",
    "\n",
    "print(stop_en[:10]); print(stop_id[:10])\n",
    "print(len(stop_id), len(stop_en))\n",
    "stop_en = set(stop_en); stop_id = set(stop_id) # Tips: selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'yang' in stop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kamu disana makan'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contoh cara menggunakan stopwords\n",
    "T = \"untuk kamu yang disana, sudah makan belum?\"\n",
    "Tokens = TextBlob(T.lower()).words # Tokenisasi \n",
    "' '.join([t for t in Tokens if t not in stop_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Menangani Slang atau Singkatan di Data Teks</font></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'1pun': 'satupun', '7an': 'tujuan', 'Dr.': 'doktor\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cara me-load daftar singkatan/slang\n",
    "with open('data/slang.dic') as f:\n",
    "    slang = json.load(f)\n",
    "\n",
    "str(slang)[:51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tujuan'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang['7an']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jangan ragu juragan langsung saja di order pajangan yang diatas besok sudah mahal\n"
     ]
    }
   ],
   "source": [
    "# Contoh Penggunaan\n",
    "T = 'jangan ragu gan, langsung saja di order pajangan yg diatas. sesok sdh mehong!'\n",
    "T = TextBlob(T).words\n",
    "\n",
    "for i, t in enumerate(T):\n",
    "    if t in slang.keys():\n",
    "        T[i] = slang[t]\n",
    "        \n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Encoding-Decoding</font></center> \n",
    "\n",
    "<ul>\n",
    "\t<li>Hal berikutnya yang perlu diperhatikan dalam memproses data teks adalah encoding-decoding.</li>\n",
    "\t<li>Contoh Encoding: ASCII, utf, latin, dsb.</li>\n",
    "\t<li>saya membahas lebih jauh tetang encoding disini:&nbsp;<br />\n",
    "\t<a href=\"https://tau-data.id/memahami-string-python/\" target=\"_blank\">https://tau-data.id/memahami-string-python/</a></li>\n",
    "\t<li>Berikut adalah sebuah contoh sederhana tantangan proses encoding-decoding ketika kita hendak memproses data yang berasal dari internet atau media sosial.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear god, please help me\n"
     ]
    }
   ],
   "source": [
    "# kita bisa menggunakan modul unidecode untuk mendapatkan representasi ASCII terdekat\n",
    "from unidecode import unidecode\n",
    "\n",
    "T = \"ḊḕḀṙ ₲ØĐ, p̾l̾e̾a̾s̾e ḧḕḶṖ ṁḕ\"\n",
    "print(unidecode(T).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satu < Tiga & © adalah simbol Copyright\n"
     ]
    }
   ],
   "source": [
    "# Kita juga bisa membersihkan posting media sosial/website dengan entitas html menggunakan fungsi \"unescape\" di modul \"html\"\n",
    "from html import unescape\n",
    "\n",
    "print(unescape('Satu &lt; Tiga&nbsp;&amp; &#169; adalah simbol Copyright'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Loading Data</font></center>\n",
    "\n",
    "* Menggunakan Data Sentimen Twitter dari: https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia\n",
    "* Silahkan kunjungi tautan diatas untuk mengakses dataset sentimen lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banyak tweet di data = 10806 tweet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>lagu bosan apa yang aku save ni huhuhuhuhuhuhu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>kita lanjutkan saja diam ini hingga kau dan ak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>doa rezeki tak putus inna haa zaa larizquna ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentimen                                              Tweet\n",
       "0        -1  lagu bosan apa yang aku save ni huhuhuhuhuhuhu...\n",
       "1        -1  kita lanjutkan saja diam ini hingga kau dan ak...\n",
       "2         1  doa rezeki tak putus inna haa zaa larizquna ma..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/ind_SA.csv\")\n",
    "print(\"Banyak tweet di data = {} tweet\".format(data.shape[0]))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet pertama = \"lagu bosan apa yang aku save ni huhuhuhuhuhuhuhuhuhuuuuuuuuuuuuuu\"\n"
     ]
    }
   ],
   "source": [
    "# Di python index dimulai dari \"0\"\n",
    "print('Tweet pertama = \"{}\"'.format(data.Tweet.loc[0]))\n",
    "data = data.sample(3000) # Agar komputasi tidak terlalu lama, kita sample datanya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Eksplorasi Data</font></center> \n",
    "\n",
    "* Preprocessing sederhana\n",
    "* Memisahkan yang positif dan negatif\n",
    "* Simpan dalam bentuk Teks\n",
    "* Visualisasi & Eksplorasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = data.sentimen.value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cleanTxt(t, lemma=True, stopword=True, slang_=True):\n",
    "    T = unescape(unidecode(str(t).lower()))\n",
    "    if lemma:\n",
    "        T = stemmer.stem(t) # ingat struktur data sastrawi\n",
    "    T = TextBlob(T).words # Tokenisasi \n",
    "    if stopword: #butuh \"stop_id\" sudah di load terlebih dahulu\n",
    "        T = [t for t in T if t not in stop_id]\n",
    "    if slang_: #butuh \"slang\" sudah di load terlebih dahulu\n",
    "        for i, t in enumerate(T):\n",
    "            if t in slang.keys():\n",
    "                T[i] = slang[t]\n",
    "    return ' '.join(T) # output string lagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'untuk kamu yang sana sudah makan'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Fungsi CleanText sederhana\n",
    "t = \"utk kamu yg disana, sdh makan belum?\"\n",
    "cleanTxt(t)\n",
    "# Analisa hasilnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [07:10,  6.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>dataCleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5594</th>\n",
       "      <td>0</td>\n",
       "      <td>cara untuk hilangkan tekanan oleh ustazah nor ...</td>\n",
       "      <td>cara hilang tekan ustazah nor hafizah musa cub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>-1</td>\n",
       "      <td>koreaelatan jerman kudampingi kau dari nol sam...</td>\n",
       "      <td>koreaelatan jerman damping kamu nol mapan eh k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>0</td>\n",
       "      <td>salah satu tujuan revolusi mental adalah membi...</td>\n",
       "      <td>salah satu tuju revolusi mental biasa diri unt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>0</td>\n",
       "      <td>lf siang lemons aku baru bikin akun myday mutu...</td>\n",
       "      <td>lf siang lemons aku baru bikin akun myday mutu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>0</td>\n",
       "      <td>tidaklah mungkin bagi matahari mendapatkan bul...</td>\n",
       "      <td>mungkin matahari bulan malam siang masing-masi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentimen                                              Tweet  \\\n",
       "5594         0  cara untuk hilangkan tekanan oleh ustazah nor ...   \n",
       "595         -1  koreaelatan jerman kudampingi kau dari nol sam...   \n",
       "8683         0  salah satu tujuan revolusi mental adalah membi...   \n",
       "4244         0  lf siang lemons aku baru bikin akun myday mutu...   \n",
       "7348         0  tidaklah mungkin bagi matahari mendapatkan bul...   \n",
       "\n",
       "                                            dataCleaned  \n",
       "5594  cara hilang tekan ustazah nor hafizah musa cub...  \n",
       "595   koreaelatan jerman damping kamu nol mapan eh k...  \n",
       "8683  salah satu tuju revolusi mental biasa diri unt...  \n",
       "4244  lf siang lemons aku baru bikin akun myday mutu...  \n",
       "7348  mungkin matahari bulan malam siang masing-masi...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing ke seluruh Tweet dan simpan dalam kolom yang baru\n",
    "data['dataCleaned'] = [cleanTxt(d.Tweet) for i,d in tqdm(data.iterrows())]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okaay nggapapaa aku minta doyoung ajaa terima kasih yaa',\n",
       " 'enaknyaaaaaa huhu aku jakarta sih semalem nge email sesuai ajar kamu baru bertanya saja dibi',\n",
       " 'tahu orang ajar bijaksana']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [d.dataCleaned for i,d in data.iterrows() if d.sentimen==1]\n",
    "neg = [d.dataCleaned for i,d in data.iterrows() if d.sentimen==-1]\n",
    "pos[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def simpan(Teks, namafile): #in Json Format\n",
    "    with open(namafile, 'w') as f:\n",
    "        for t in Teks:\n",
    "            f.write(t+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpan(pos, 'tweet-positif.txt')\n",
    "simpan(neg, 'tweet-negatif.txt')\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Text Analytics - Voyant Tools</font></center> \n",
    "\n",
    "### https://voyant-tools.org/\n",
    "\n",
    "* WordCloud, Word Links, Word Tree\n",
    "\n",
    "<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Unsupervised Sentiment Analysis</font></center> \n",
    "\n",
    "Algoritma:\n",
    "1. Load Lexicon Positif dan Negatif\n",
    "2. Hitung jumlah kata positif dan negatif di data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Fungsi Load Lexicon\n",
    "def loadLexicon(file):\n",
    "    df=open(file,\"r\",encoding=\"utf-8\", errors='replace')\n",
    "    data=df.readlines();df.close()\n",
    "    return [d.strip().lower() for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a+', 'acungan jempol', 'adaptif', 'adil', 'afinitas', 'afirmasi', 'agilely', 'agung', 'ahli', 'ahlinya']\n",
      "['abnormal', 'absurd', 'acak', 'acak-acakan', 'acuh', 'acuh tak acuh', 'adiktif', 'adil', 'agresi', 'agresif']\n",
      "['belum', 'bukan', 'engga', 'enggak', 'ga', 'harusnya', 'nggak', 'tak', 'tidak']\n"
     ]
    }
   ],
   "source": [
    "fpos, fneg, fnegasi = 'data/s-pos.txt', 'data/s-neg.txt', 'data/s-negasi.txt'\n",
    "positif, negatif, negasi = loadLexicon(fpos), loadLexicon(fneg), loadLexicon(fnegasi)\n",
    "print(positif[:10])\n",
    "print(negatif[:10])\n",
    "print(negasi[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prediksiSentiment(kalimat, positif, negatif, negasi):\n",
    "    # Naive Approach, nanti akan kita diskusikan bagaimana improvisasi fungsi sederhana ini\n",
    "    posWords = []\n",
    "    negWords = [w for w in negatif if w in kalimat]\n",
    "    for w in positif:\n",
    "        if w in kalimat:\n",
    "            negated = False\n",
    "            for n in negasi:\n",
    "                if n+' '+w in kalimat:\n",
    "                    negWords.append(n+' '+w)\n",
    "                    negated = True\n",
    "                    break\n",
    "            if not negated:\n",
    "                posWords.append(w)\n",
    "    nPos, nNeg = len(posWords), len(negWords)\n",
    "    if nPos>nNeg:\n",
    "        return 1\n",
    "    if nPos<nNeg:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "T = \"mie ayam ini enak\"\n",
    "prediksiSentiment(T, positif, negatif, negasi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Discussion: Strength & weakness, Error & Model Analysis + Interpretation</font></center> \n",
    "\n",
    "1. Apa kelebihan dan kekurangannya?\n",
    "2. Apa yang bisa dilakukan untuk mencoba memperbaikinya?\n",
    "3. Interpretasi hasil Positif dan Negatifnya.\n",
    "4. Mari analisa data-data yang salah diprediksi.\n",
    "\n",
    "## <center><font color=\"blue\">Metode unsupervised yang lain</font></center>\n",
    "\n",
    "* Bing Liu’s, MPQA subjectivity, AFINN, SentiWordNet, VADER, dll\n",
    "* Contoh penggunaan Vader & Afinn: https://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project03%20-%20Sentiment%20Analysis%20Unsupervised%20Lexical%20Models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [00:00, 3250.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>dataCleaned</th>\n",
       "      <th>unsupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5594</th>\n",
       "      <td>0</td>\n",
       "      <td>cara untuk hilangkan tekanan oleh ustazah nor ...</td>\n",
       "      <td>cara hilang tekan ustazah nor hafizah musa cub...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>-1</td>\n",
       "      <td>koreaelatan jerman kudampingi kau dari nol sam...</td>\n",
       "      <td>koreaelatan jerman damping kamu nol mapan eh k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>0</td>\n",
       "      <td>salah satu tujuan revolusi mental adalah membi...</td>\n",
       "      <td>salah satu tuju revolusi mental biasa diri unt...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>0</td>\n",
       "      <td>lf siang lemons aku baru bikin akun myday mutu...</td>\n",
       "      <td>lf siang lemons aku baru bikin akun myday mutu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>0</td>\n",
       "      <td>tidaklah mungkin bagi matahari mendapatkan bul...</td>\n",
       "      <td>mungkin matahari bulan malam siang masing-masi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentimen                                              Tweet  \\\n",
       "5594         0  cara untuk hilangkan tekanan oleh ustazah nor ...   \n",
       "595         -1  koreaelatan jerman kudampingi kau dari nol sam...   \n",
       "8683         0  salah satu tujuan revolusi mental adalah membi...   \n",
       "4244         0  lf siang lemons aku baru bikin akun myday mutu...   \n",
       "7348         0  tidaklah mungkin bagi matahari mendapatkan bul...   \n",
       "\n",
       "                                            dataCleaned  unsupervised  \n",
       "5594  cara hilang tekan ustazah nor hafizah musa cub...            -1  \n",
       "595   koreaelatan jerman damping kamu nol mapan eh k...             0  \n",
       "8683  salah satu tuju revolusi mental biasa diri unt...            -1  \n",
       "4244  lf siang lemons aku baru bikin akun myday mutu...             0  \n",
       "7348  mungkin matahari bulan malam siang masing-masi...            -1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediksi ke seluruh data\n",
    "data['unsupervised'] = [prediksiSentiment(t.Tweet, positif, negatif, negasi) for i,t in tqdm(data.iterrows())]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.53      0.46       823\n",
      "           0       0.58      0.45      0.51      1456\n",
      "           1       0.36      0.40      0.38       721\n",
      "\n",
      "    accuracy                           0.46      3000\n",
      "   macro avg       0.45      0.46      0.45      3000\n",
      "weighted avg       0.48      0.46      0.46      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(data.sentimen, data.unsupervised))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"red\">Hasilnya jelek sekali, apakah pendekatan ini seburuk itu?</font></center> \n",
    "\n",
    "## Mari kita analisa lebih lanjut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betul = [d.Tweet for i,d in data.iterrows() if d.sentimen==d.unsupervised]\n",
    "salah = [d.Tweet for i,d in data.iterrows() if d.sentimen!=d.unsupervised]\n",
    "simpan(betul, 'unsupervised-betul.txt')\n",
    "simpan(salah, 'unsupervised-salah.txt')\n",
    "\"Done\" # Lanjut ke Voyant Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Representasi Dokumen/Teks</font></center> \n",
    "\n",
    "<img alt=\"\" src=\"images/3_Bentuk umum representasi dokumen.JPG\" style=\"height: 294px ; width: 620px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">tf-idf: Term Frequency - Inverse Document Frequency</font></center> \n",
    "\n",
    "<img alt=\"\" src=\"images/toydata_vsm.png\" />\n",
    "\n",
    "* Menurut http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "* default formula tf-idf yang digunakan sk-learn adalah:\n",
    "\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> linear_tf, Smooth IDF\n",
    "* namun kita merubahnya menjadi:\n",
    "* $tfidf = tf * log(\\frac{N}{df})$ ==> linear_tf, Non Smooth IDF\n",
    "* $tfidf = (1+log(tf)) * log(\\frac{N}{df+1})$ ==> sublinear_tf, Smooth IDF\n",
    "* $tfidf = (1+log(tf)) * log(\\frac{N}{df})$ ==> sublinear_tf, Non Smooth IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Best-Match-Formula-:-BM25\">Best-Match Formula : BM25</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_bm25_simple.png\" style=\"height: 123px; width: 300px;\" /></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>di IR nilai b dan k yang optimal adalah :&nbsp;<strong> <em>b</em> = 0.75&nbsp; dan k = [1.2 - 2.0]&nbsp; &nbsp;</strong><br />\n",
    "\tref:&nbsp;<em>Christopher, D. M., Prabhakar, R., &amp; Hinrich, S. C. H. &Uuml;. T. Z. E. (2008). Introduction to information retrieval.&nbsp;An Introduction To Information Retrieval,&nbsp;151, 177.</em></li>\n",
    "\t<li>Tapi kalau untuk TextMining (clustering) nilai <strong>k optimal adalah 20, nilai b = sembarang (boleh = 0.75)</strong><br />\n",
    "\tref:&nbsp;<em>Whissell, J. S., &amp; Clarke, C. L. (2011). Improving document clustering using Okapi BM25 feature weighting.&nbsp;Information retrieval,&nbsp;14(5), 466-487.</em></li>\n",
    "\t<li><strong>avgDL </strong>adalah rata-rata panjang dokumen di seluruh dataset dan <strong>DL </strong>adalah panjang dokumen D.<br />\n",
    "\thati-hati, ini berbeda dengan &nbsp;tf-idf MySQL diatas.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan modul SciKit untuk merubah data tidak terstruktur ke VSM\n",
    "# Scikit implementation http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Parameter: lowercase=True, smooth_idf= True, sublinear_tf=True,  ngram_range=(1, 2), max_df=0.90, min_df=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'udin76': 35, 'minum': 21, 'kopi': 14, 'pagi': 24, 'sambil': 28, 'makan': 18, 'pisang': 25, 'goreng': 11, 'is': 12, 'the': 33, 'best': 6, 'belajar': 4, 'nlp': 22, 'dan': 8, 'text': 32, 'mining': 20, 'ternyata': 31, 'seru': 29, 'banget': 3, 'sadiezz': 27, 'sudah': 30, 'lumayan': 17, 'lama': 15, 'bingits': 7, 'tukang': 34, 'bakso': 2, 'belum': 5, 'lewat': 16, 'aduh': 0, 'ga': 10, 'mie': 19, 'ayam': 1, 'p4k4i': 23, 'kesyap': 13, 'please': 26, 'deh': 9}\n"
     ]
    }
   ],
   "source": [
    "# Variasi pembentukan matriks VSM:\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "\n",
    "D = [d1, d2, d3, d4]\n",
    "# Jika kita menggunakan cara biasa: \n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True)\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'udin76': 69, 'minum': 41, 'kopi': 27, 'pagi': 47, 'sambil': 55, 'makan': 34, 'pisang': 50, 'goreng': 21, 'is': 23, 'the': 65, 'best': 13, 'udin76 minum': 70, 'minum kopi': 42, 'kopi pagi': 28, 'pagi pagi': 48, 'pagi sambil': 49, 'sambil makan': 56, 'makan pisang': 36, 'pisang goreng': 51, 'goreng is': 22, 'is the': 24, 'the best': 66, 'belajar': 9, 'nlp': 43, 'dan': 16, 'text': 63, 'mining': 39, 'ternyata': 61, 'seru': 57, 'banget': 6, 'sadiezz': 54, 'belajar nlp': 10, 'nlp dan': 44, 'dan text': 17, 'text mining': 64, 'mining ternyata': 40, 'ternyata seru': 62, 'seru banget': 58, 'banget sadiezz': 8, 'sudah': 59, 'lumayan': 32, 'lama': 29, 'bingits': 14, 'tukang': 67, 'bakso': 4, 'belum': 11, 'lewat': 31, 'sudah lumayan': 60, 'lumayan lama': 33, 'lama bingits': 30, 'bingits tukang': 15, 'tukang bakso': 68, 'bakso belum': 5, 'belum lewat': 12, 'aduh': 0, 'ga': 19, 'mie': 37, 'ayam': 2, 'p4k4i': 45, 'kesyap': 25, 'please': 52, 'deh': 18, 'aduh ga': 1, 'ga banget': 20, 'banget makan': 7, 'makan mie': 35, 'mie ayam': 38, 'ayam p4k4i': 3, 'p4k4i kesyap': 46, 'kesyap please': 26, 'please deh': 53}\n"
     ]
    }
   ],
   "source": [
    "# N-Grams VSM\n",
    "# Bermanfaat untuk menangkap frase kata, misal: \"ga banget\", \"pisang goreng\", dsb\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.70710678 0.         0.         0.         0.70710678\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seru banget': 0,\n",
       " 'seru': 1,\n",
       " 'the best': 2,\n",
       " 'lama': 3,\n",
       " 'text mining': 4,\n",
       " 'nlp': 5,\n",
       " 'ayam': 6}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary based VSM\n",
    "# Bermanfaat untuk menghasilkan hasil analisa yang \"bersih\"\n",
    "# variasi 2\n",
    "D = [d1,d2,d3,d4]\n",
    "Vocab = {'seru banget':0, 'seru':1, 'the best':2, 'lama':3, 'text mining':4, 'nlp':5, 'ayam':6}\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=Vocab)\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(vsm.toarray())\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    \n",
    "# <center><strong><font color=\"blue\">Supervised Sentiment Analysis</font></strong></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk VSM-nya\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,smooth_idf= True, sublinear_tf=True, \n",
    "                                   ngram_range=(1, 2), max_df=0.95, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 4280) (600, 4280)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 99\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['dataCleaned'], data['sentimen'], test_size=0.2, random_state=seed)\n",
    "x_train = tfidf_vectorizer.fit_transform(x_train) # \"Fit_Transform\"\n",
    "x_test = tfidf_vectorizer.transform(x_test) # Perhatikan disini hanya \"Transform\"\n",
    "\n",
    "print(x_train.shape, x_test.shape) # Jumlah kolom Sama ==> ini penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      0.39      0.43       161\n",
      "           0       0.63      0.80      0.71       310\n",
      "           1       0.57      0.34      0.43       129\n",
      "\n",
      "    accuracy                           0.59       600\n",
      "   macro avg       0.56      0.51      0.52       600\n",
      "weighted avg       0.58      0.59      0.57       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "from sklearn import svm\n",
    "\n",
    "dSVM = svm.SVC(kernel='linear', decision_function_shape='ovo') # oneversus one SVM\n",
    "dSVM.fit(x_train, y_train)\n",
    "\n",
    "y_SVM = dSVM.predict(x_test)\n",
    "print(classification_report(y_test, y_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasilnya rendah? ... Hhmmmm ... Mari coba optimalkan model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['dataCleaned'], data['sentimen'], test_size=0.2, random_state=seed)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "pipeSVM = make_pipeline(TfidfVectorizer(), svm.SVC())\n",
    "#print(sorted(pipeSVM.get_params().keys()))\n",
    "\n",
    "paramsSVM = {}\n",
    "paramsSVM['tfidfvectorizer__min_df'] = [5, 10, 30]\n",
    "paramsSVM['tfidfvectorizer__max_df'] = [0.5, 0.75, 0.95]\n",
    "paramsSVM['tfidfvectorizer__smooth_idf'] = [True] # [True, False]\n",
    "paramsSVM['tfidfvectorizer__sublinear_tf'] = [True] # [True, False]\n",
    "paramsSVM['tfidfvectorizer__ngram_range'] = [(1, 1), (1, 2), (1,3)]\n",
    "paramsSVM['svc__C'] = [0.1, 10, 100] #sp.stats.uniform(scale=1)\n",
    "paramsSVM['svc__gamma'] = [1.0, 0.1, 0.001]\n",
    "paramsSVM['svc__kernel'] = ['rbf', 'poly', 'sigmoid', 'linear']\n",
    "paramsSVM['svc__decision_function_shape'] = ['ovo', 'ovr']\n",
    "\n",
    "gridsvmCV = GridSearchCV(pipeSVM, paramsSVM, cv=5, scoring='accuracy', verbose=1, n_jobs=-3)\n",
    "gridsvmCV.fit(x_train, y_train) # hati-hati disini x_train harus Text\n",
    "print(gridsvmCV.best_score_)\n",
    "print(gridsvmCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mungkin Modelnya yang tidak mampu mengklasifikasikan?\n",
    "\n",
    "## Kita coba Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier()\n",
    "NN.fit(x_train, y_train)\n",
    "y_NN = NN.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_NN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Dengan cara yang sama seperti tadi, coba analisa data yang salah di prediksi</font></center> \n",
    "\n",
    "### Coba juga ke seluruh data dan variasi preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Di kesempatan lain kita bisa coba dengan model Deep Learning</font></center> \n",
    "\n",
    "<p><img alt=\"\" src=\"images/5_DeepLearning.png\" style=\"width: 690px ; height: 777px\" /></p>\n",
    "\n",
    "<p><big>Yang menjadi pembeda utama DL dengan ML adalah DL &quot;<em>Learning representations from data</em>&quot;. Misal Word Embedding (bandingkan dengan VSM di Machine Learning).<br />\n",
    "Makna &quot;Deep&quot; di DL sendiri bermakna &quot;successive layers of representations&quot; biasa juga disebut sebagai&nbsp;<em>layered representations learning</em> atau <em>hierarchical representations learning</em>.</big></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: Long Short Term memory\n",
    "\n",
    "### https://tau-data.id/lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Better Unsupervised Sentiment Analysis</font></center>\n",
    "\n",
    "<p><img src=\"images/satriadata2021/unsupervised_sentimen_analysis.png\" /></p>\n",
    "\n",
    "* Feature Engineering, Lexicon Based, & Bayesian Model\n",
    "* image source: \n",
    " - http://pasaentuciudad.com.mx/sentiment-analysis%E2%80%8A-%E2%80%8Acomparing-3-common-approaches-naive-bayes-lstm-and-vader/\n",
    " - https://www.emerald.com/insight/content/doi/10.1016/j.aci.2019.11.003/full/html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Improvement Implementasi di Industri</font></center>\n",
    "\n",
    "* **Hierarchical Classification**: Aspect-Based Sentiment Analysis\n",
    "* **Hybrid Model**: Sentimen Analysis ==> Topic Modelling\n",
    "\n",
    "<p><img src=\"images/Aspect-based-sentiment.png\" alt=\"\" width=\"399\" height=\"187\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Supplementary\">Supplementary</h2>\n",
    "\n",
    "<p>* Negasi suatu kata bukan berarti memiliki sentimen kebalikannya. Misal &quot;jelek&quot; dan &quot;tidak jelek&quot; (terrible vs not terrible).</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/negation_sentiments.png\" /></p>\n",
    "\n",
    "<p>[*]. Zhu, X., Guo, H., Mohammad, S., &amp; Kiritchenko, S. (2014). An empirical study on the effect of negation words on sentiment. In&nbsp;<i>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>&nbsp;(Vol. 1, pp. 304-313).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Makna positive/negative atau pro/kontra subjective (bias) terhadap user.\n",
    "* StopWords removal in general is a bad idea\n",
    "* learn the lingo in your topic, sentiment expressions are different across fields, languages, and regions.\n",
    "* Sarcasm perlu konteks untuk di deteksi dengan tepat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Feature-Engineering/Extraction\">Feature Engineering/Extraction</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Ketimbang pemilihan model yang optimal, beberapa literature sudah melaporkan bahwa feature engineering/extraction lebih efektif [1].</li>\n",
    "\t<li>Selain itu, pendekatan semantic dalam FE juga lebih plausible untuk dilakukan.</li>\n",
    "\t<li>Tabel berikut adalah contoh FE yang bisa dilakukan spesifik terhadap model SA.</li>\n",
    "\t<li><img alt=\"\" src=\"images/SA_Analysis_Features.png\" style=\"width: 544px; height: 425px;\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> End of Module.\n",
    "\n",
    "<hr />\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
