{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><strong>Natural Language Processing and Text Mining (NLPTM)<br> Unstructured Data Analysis (UDA)*</strong></center>\n",
    "    \n",
    "## <center><strong><font color=\"blue\">NLPTM-05: Representasi Dokumen</font></strong></center>\n",
    "<center><img alt=\"\" src=\"images/SocMed.png\"/> </center>\n",
    "    \n",
    "## <center>(C) Taufik Sutanto - 2021 <br><strong><font color=\"blue\"> tau-data Indonesia ~ https://tau-data.id/uda/</font></strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### contoh dataset dokumen yang cukup tenar: &quot;20 NewsGroup&quot;\n",
    "\n",
    "<img alt=\"\" src=\"images/6_20News.jpg\" style=\"height: 300px ; width: 533px\" />\n",
    "\n",
    "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\" target=\"_blank\">http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups</a>\n",
    "\n",
    "<strong>Categories </strong>\n",
    "<pre>\n",
    "[&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;comp.os.ms-windows.misc&#39;, &#39;comp.sys.ibm.pc.hardware&#39;, &#39;comp.sys.mac.hardware&#39;,\n",
    " &#39;comp.windows.x&#39;, &#39;misc.forsale&#39;, &#39;rec.autos&#39;, &#39;rec.motorcycles&#39;, &#39;rec.sport.baseball&#39;, &#39;rec.sport.hockey&#39;,\n",
    " &#39;sci.crypt&#39;, &#39;sci.electronics&#39;, &#39;sci.med&#39;, &#39;sci.space&#39;, &#39;soc.religion.christian&#39;, &#39;talk.politics.guns&#39;,\n",
    " &#39;talk.politics.mideast&#39;, &#39;talk.politics.misc&#39;, &#39;talk.religion.misc&#39;]</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "try:\n",
    "    f = open('data/20newsgroups.pckl', 'rb')\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "except:\n",
    "    categories = ['sci.med', 'talk.politics.misc',  'rec.autos']\n",
    "    data = fetch_20newsgroups(categories=categories,remove=('headers', 'footers', 'quotes'))\n",
    "    f = open('data/20newsgroups.pckl', 'wb')\n",
    "    pickle.dump(data, f)\n",
    "    f.close()\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n",
      "I want to get a car alarm and I am thinking about getting an Ungo Box.\n",
      "    Does anyone out there have any knowledge or experience with any of\n",
      "    these alarms?  How about price ranges for the different models?\n",
      "    Are these good car alarms?  Please email me any responces.\n",
      "\n",
      "                cak3@ns3.lehigh.edu\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "# Contoh cara mengakses datanya\n",
    "print( type(data) )\n",
    "print(dir(data))\n",
    "print(data.data[0]) # Dokumen pertama\n",
    "print(data.target_names[0]) # Dokumen pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)  # uncomment if you wanna see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh Label 3 dokumen pertama:  [0 2 0]\n",
      "Contoh 2 dokumen pertama:  ['I want to get a car alarm and I am thinking about getting an Ungo Box.\\n    Does anyone out there have any knowledge or experience with any of\\n    these alarms?  How about price ranges for the different models?\\n    Are these good car alarms?  Please email me any responces.\\n\\n                cak3@ns3.lehigh.edu', 'I did hear this question asked during a radio news update of the case.  (They\\nwere talking about the ongoing trial and had some audio clips).  Immediately\\nafter the defense attorney asked the question, there was an \"Objection!\" heard\\nin the background.  The clip ended at that point so I don\\'t know if the\\nobjection was upheld.  I can\\'t imagine NC is *that* bad. \\n\\nThis I didn\\'t hear as an audio clip but heard it reported a number of times on\\nnews stories both during and after the trial.  Now the \"we did it on purpose\"\\nthing is stretching, I think it was something more like--he had it coming.  If\\nsomebody else remebers better than I on this second point, feel free to\\nclarify.  \\n        \\nFrank\\n\\n--------------------------------------------------------------------------\\n\\nFrank R. Chloupek \\nCHLOUPEK@ohstpy.mps.ohio-state.edu \\nDepartment of Physics -- *The* Ohio State University\\n(Not just any Ohio State University) ']\n"
     ]
    }
   ],
   "source": [
    "# Rubah struktur data diatas ke dalam bentuk struktur data sederhana: \"list of documents\"\n",
    "Y = data.target # List: 0 = class 1, 1 = class 2, ... dst\n",
    "print('Contoh Label 3 dokumen pertama: ',Y[:3])\n",
    "X = [doc for doc in data.data] # setiap elemen dalam list adalah dokumen\n",
    "print('Contoh 2 dokumen pertama: ', X[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan modul SciKit untuk merubah data tidak terstruktur ke VSM\n",
    "# Scikit implementation http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1157, 14979) (496, 14979)\n",
      "[0.26663231 0.25560483 0.26663231 0.26663231 0.24705126 0.21042443\n",
      " 0.11349002 0.229035   0.20246517 0.24006248 0.18101388 0.15513474\n",
      " 0.24705126 0.26663231 0.19204136 0.26663231 0.17357146 0.15444405\n",
      " 0.20874217 0.24006248]\n",
      "[ 1861 13057  1091  7424 12324  1860  4209 11645  3414 13056   786  5523\n",
      " 10636  4399  9647 11382  6346  1086  7422   914]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english',smooth_idf= True, sublinear_tf=True, \n",
    "                                   ngram_range=(1, 2), max_df=0.90, min_df=2)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "x_train = tfidf_vectorizer.fit_transform(x_train) # \"Fit_Transform\"\n",
    "x_test = tfidf_vectorizer.transform(x_test) # Perhatikan disini hanya \"Transform\"\n",
    "\n",
    "print(x_train.shape, x_test.shape) # Jumlah kolom Sama ==> ini penting\n",
    "print(x_train[0].data) # Values baris pertama ... Pelajari ADSP-03 untuk detailnya\n",
    "print(x_train[0].indices) # Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi =  0.907258064516129\n"
     ]
    }
   ],
   "source": [
    "NN = MLPClassifier(activation='logistic', solver='adam', hidden_layer_sizes=(30, 20), random_state=seed)# 2 layers 30 neurons and 20 neurons\n",
    "NN.fit(x_train, y_train)\n",
    "y_NN = NN.predict(x_test)\n",
    "print('Akurasi = ', accuracy_score(y_test, y_NN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1. preprocess Text\n",
    "2. Rubah dalam bentuk \"List of Tokens\"\n",
    "3. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1653/1653 [00:12<00:00, 132.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583 [['i', 'want', 'to', 'get', 'a', 'car', 'alarm', 'and', 'i', 'am', 'thinking', 'about', 'getting', 'an', 'ungo', 'box', 'does', 'anyone', 'out', 'there', 'have', 'any', 'knowledge', 'or', 'experience', 'with', 'any', 'of', 'these', 'alarms', 'how', 'about', 'price', 'ranges', 'for', 'the', 'different', 'models', 'are', 'these', 'good', 'car', 'alarms', 'please', 'email', 'me', 'any', 'responces', 'cak3', 'ns3.lehigh.edu'], ['i', 'did', 'hear', 'this', 'question', 'asked', 'during', 'a', 'radio', 'news', 'update', 'of', 'the', 'case', 'they', 'were', 'talking', 'about', 'the', 'ongoing', 'trial', 'and', 'had', 'some', 'audio', 'clips', 'immediately', 'after', 'the', 'defense', 'attorney', 'asked', 'the', 'question', 'there', 'was', 'an', 'objection', 'heard', 'in', 'the', 'background', 'the', 'clip', 'ended', 'at', 'that', 'point', 'so', 'i', 'do', \"n't\", 'know', 'if', 'the', 'objection', 'was', 'upheld', 'i', 'ca', \"n't\", 'imagine', 'nc', 'is', 'that', 'bad', 'this', 'i', 'did', \"n't\", 'hear', 'as', 'an', 'audio', 'clip', 'but', 'heard', 'it', 'reported', 'a', 'number', 'of', 'times', 'on', 'news', 'stories', 'both', 'during', 'and', 'after', 'the', 'trial', 'now', 'the', 'we', 'did', 'it', 'on', 'purpose', 'thing', 'is', 'stretching', 'i', 'think', 'it', 'was', 'something', 'more', 'like', 'he', 'had', 'it', 'coming', 'if', 'somebody', 'else', 'remebers', 'better', 'than', 'i', 'on', 'this', 'second', 'point', 'feel', 'free', 'to', 'clarify', 'frank', 'frank', 'r', 'chloupek', 'chloupek', 'ohstpy.mps.ohio-state.edu', 'department', 'of', 'physics', 'the', 'ohio', 'state', 'university', 'not', 'just', 'any', 'ohio', 'state', 'university']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Rubah bentuk data seperti yang dibutuhkan genSim\n",
    "# Bisa juga dilakukan dengan memodifikasi fungsi \"cleanText\" (agar lebih efisien)\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_we = []\n",
    "for doc in tqdm(X):\n",
    "    tok = [str(w).lower().strip() for w in TextBlob(doc).words]\n",
    "    if len(tok)>2:\n",
    "        data_we.append(tok)\n",
    "    \n",
    "print(len(data_we), data_we[:2]) # Hasilnya tidak bersih karena kita ndak/belum preprocess Text ==> Pelajari NLPTM 01 & 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 15:52:46.090242 13756 base_any2vec.py:1343] EPOCH - 1 : supplied example count (0) did not equal expected count (1583)\n",
      "W1019 15:52:46.092200 13756 base_any2vec.py:1348] EPOCH - 1 : supplied raw word count (0) did not equal expected count (322385)\n",
      "W1019 15:52:46.101188 13756 base_any2vec.py:1343] EPOCH - 2 : supplied example count (0) did not equal expected count (1583)\n",
      "W1019 15:52:46.107549 13756 base_any2vec.py:1348] EPOCH - 2 : supplied raw word count (0) did not equal expected count (322385)\n",
      "W1019 15:52:46.121504 13756 base_any2vec.py:1343] EPOCH - 3 : supplied example count (0) did not equal expected count (1583)\n",
      "W1019 15:52:46.123497 13756 base_any2vec.py:1348] EPOCH - 3 : supplied raw word count (0) did not equal expected count (322385)\n",
      "W1019 15:52:46.128489 13756 base_any2vec.py:1343] EPOCH - 4 : supplied example count (0) did not equal expected count (1583)\n",
      "W1019 15:52:46.129482 13756 base_any2vec.py:1348] EPOCH - 4 : supplied raw word count (0) did not equal expected count (322385)\n",
      "W1019 15:52:46.140452 13756 base_any2vec.py:1343] EPOCH - 5 : supplied example count (0) did not equal expected count (1583)\n",
      "W1019 15:52:46.142446 13756 base_any2vec.py:1348] EPOCH - 5 : supplied raw word count (0) did not equal expected count (322385)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "L = 100 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_wv = Word2Vec(data_we, min_count=5, size=L, window = 5, workers= -2)\n",
    "# min_count adalah jumlah kata minimal yang muncul di corpus\n",
    "# \"size\" adalah Dimensionality of the word vectors \n",
    "# (menurut beberapa literature untuk text disarankan 300-500)\n",
    "# \"window\" adalah jarak maximum urutan kata yang di pertimbangkan\n",
    "# workers = jumlah prosesor yang digunakan untuk menjalankan word2vec\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!...\n"
     ]
    }
   ],
   "source": [
    "# Save model DL ndak bisa pakai Pickle\n",
    "# di data yang sebenarnya (i.e. besar) Gensim sering membutuhkan waktu cukup lama\n",
    "# Untungnya kita bisa menyimpan dan me-load kembali hasil perhitungan model word2vec, misal\n",
    "model_wv.save('data/model_w2v')\n",
    "model_wv = Word2Vec.load('data/model_w2v')\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[ 9.36206838e-04  1.13226136e-03 -4.09837440e-03 -4.82797995e-03\n",
      " -2.73836544e-03  2.05509155e-03 -9.56754724e-04 -2.75190058e-03\n",
      " -2.21861247e-03  3.08032706e-03  3.09082610e-03  3.89480661e-03\n",
      " -4.47366294e-03  2.70126900e-03  4.74721455e-04  4.67735197e-04\n",
      "  3.55406402e-04 -4.29088902e-03 -4.08294750e-03 -3.65177193e-03\n",
      "  5.49541801e-05  3.62973916e-03  1.24706153e-03  1.58222788e-03\n",
      "  4.42425735e-05 -1.90756668e-03 -3.84107255e-03  2.40757805e-03\n",
      "  3.84641535e-05 -2.88823969e-03 -4.30307118e-05  3.17429146e-03\n",
      "  2.62589846e-03 -7.84015632e-04  1.07565068e-03 -1.62788713e-03\n",
      "  2.74186558e-03  1.50121967e-04 -1.52110599e-03  8.86697206e-04\n",
      "  2.34366790e-03  2.74316780e-03  3.06099677e-03 -3.26653681e-04\n",
      " -4.77064867e-04  4.36128397e-03 -3.01153772e-03 -1.07246544e-03\n",
      "  1.28297135e-03 -9.38369776e-04  4.47690114e-03  3.96923897e-05\n",
      " -2.35148496e-03  2.86965794e-03 -3.84591194e-03 -2.17800587e-03\n",
      " -2.73051043e-03 -3.65985837e-03 -3.25493445e-03  1.05827337e-03\n",
      " -2.97082379e-03 -4.18877881e-03 -1.84660929e-03  4.16277116e-03\n",
      " -4.76217084e-03  1.02095546e-04  3.49889859e-03  4.09910548e-03\n",
      " -1.38016383e-03  1.51322049e-04 -1.58651324e-04  4.70331917e-03\n",
      " -3.02405236e-03 -3.48781887e-03  4.75623924e-03 -3.07831448e-04\n",
      "  6.05522713e-04 -1.06216746e-03  4.54702741e-03  5.01561008e-05\n",
      " -1.29443461e-05 -2.58817966e-03  1.37949624e-04 -2.54348293e-03\n",
      "  4.48420551e-03 -1.67261821e-03 -2.71078013e-03  4.10282472e-03\n",
      " -4.12354898e-03 -1.03199622e-03 -5.13683131e-04  4.07079729e-04\n",
      "  1.29123058e-04 -2.49767047e-03 -4.64316597e-03  4.50239796e-03\n",
      "  1.34966476e-03  2.40200810e-04  3.07234656e-03  4.00500372e-03]\n"
     ]
    }
   ],
   "source": [
    "# Melihat vector suatu kata\n",
    "vektor = model_wv.wv.__getitem__(['car'])\n",
    "print(len(vektor[0])) # Panjang vektor keseluruhan = jumlah neuron yang digunakan\n",
    "print(vektor[0]) # 5 elemen pertama dari vektornya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('failed', 0.4082484841346741),\n",
       " ('russians', 0.34772080183029175),\n",
       " ('since', 0.3098464906215668),\n",
       " ('comprehensive', 0.3061678409576416),\n",
       " ('backup', 0.2991780638694763),\n",
       " ('deleted', 0.2983536720275879),\n",
       " ('and/or', 0.29487353563308716),\n",
       " ('soda.berkeley.edu', 0.29104846715927124),\n",
       " ('manta', 0.28728359937667847),\n",
       " ('determine', 0.2870174050331116)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_wv.wv.most_similar('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08571026\n",
      "0.17774886\n",
      "0.99999994\n"
     ]
    }
   ],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_wv.wv.similarity('car', 'supra'))\n",
    "print(model_wv.wv.similarity('car', 'alarm'))\n",
    "print(model_wv.wv.similarity('car', 'car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error! kata \" mobil \" tidak ada di training data\n"
     ]
    }
   ],
   "source": [
    "# error jika kata tidak ada di training data\n",
    "# beckman bukan beckmans ==> hence di Word Embedding PreProcessing harus thourough\n",
    "\n",
    "kata = 'mobil'\n",
    "try:\n",
    "    print(model_wv.wv.most_similar(kata))\n",
    "except:\n",
    "    print('error! kata \"',kata,'\" tidak ada di training data')\n",
    "# ini salah satu kelemahan Word2Vec\n",
    "# Word2Vec ndak bisa \"predict\" kata yang ndak ada di vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 11:35:34.765610 13756 base_any2vec.py:1168] Effective 'alpha' higher than previous training cycles\n",
      "W1019 11:35:34.768602 13756 base_any2vec.py:1343] EPOCH - 1 : supplied example count (0) did not equal expected count (3)\n",
      "W1019 11:35:34.770597 13756 base_any2vec.py:1343] EPOCH - 2 : supplied example count (0) did not equal expected count (3)\n",
      "W1019 11:35:34.772590 13756 base_any2vec.py:1343] EPOCH - 3 : supplied example count (0) did not equal expected count (3)\n",
      "W1019 11:35:34.774586 13756 base_any2vec.py:1343] EPOCH - 4 : supplied example count (0) did not equal expected count (3)\n",
      "W1019 11:35:34.776580 13756 base_any2vec.py:1343] EPOCH - 5 : supplied example count (0) did not equal expected count (3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('swollen', 0.00017699115), ('steroids', 0.00017699115), ('irrational', 0.00017699115), ('pulled', 0.00017699115), ('egg', 0.00017699115), ('edge', 0.00017699115), (\"you're\", 0.00017699115), ('quit', 0.00017699115), ('dare', 0.00017699115), ('signature', 0.00017699115)]\n"
     ]
    }
   ],
   "source": [
    "# \"predict\" vector for new data without re-training from the beginning\n",
    "d1 = ['mobil','baru', 'pakai', 'nvidia','gpu','rtx', 'untuk', 'AI', 'nya']\n",
    "d2 = ['deep','learning','computation','mostly', 'on', 'gpu']\n",
    "d3 = ['the','rtx','gpu','capable','super','sampling','ssdl']\n",
    "D = [d1,d2,d3]\n",
    "model_wv.train(D, total_examples=len(D), epochs=model_wv.epochs)\n",
    "print(model_wv.predict_output_word('mobil'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\" FastText-(Facebook-2016)\">&nbsp;FastText (Facebook-2016)</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Menggunakan Sub-words: app, ppl, ple - apple</li>\n",
    "\t<li>Paper:&nbsp;https://arxiv.org/abs/1607.04606&nbsp;&nbsp;</li>\n",
    "\t<li>Website:&nbsp;https://fasttext.cc/</li>\n",
    "\t<li>Source:&nbsp;https://github.com/facebookresearch/fastText&nbsp;</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning agak lama ... Computational complexity FastText > Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "L = 300 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_FT = FastText(data_we, size=L, window=4, min_count=1, workers=2)\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canion', 0.9992823600769043),\n",
       " ('cast', 0.9992755055427551),\n",
       " ('called-upon', 0.9992024302482605),\n",
       " ('reckon', 0.9991419315338135),\n",
       " ('casio', 0.9990610480308533),\n",
       " ('coronas', 0.9990547895431519),\n",
       " ('rehash', 0.9990296363830566),\n",
       " ('wal-mart', 0.9990094304084778),\n",
       " ('comas', 0.9989776015281677),\n",
       " ('aw', 0.9989054203033447)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_FT.wv.most_similar('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.99439853\n",
      "0.991306\n"
     ]
    }
   ],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_FT.wv.similarity('car', 'car'))\n",
    "print(model_FT.wv.similarity('car', 'carpal'))\n",
    "print(model_FT.wv.similarity('car', 'carter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('belts', 0.9998795986175537), ('meclomen', 0.9998269081115723), ('benzocaine', 0.9996403455734253), ('non-citizen', 0.9995907545089722), ('syndome', 0.9995849132537842), ('bench', 0.9995487928390503), ('treffeisen', 0.999538004398346), ('men', 0.9995251297950745), ('strengthen', 0.9995245933532715), ('outcome', 0.9994848966598511)]\n"
     ]
    }
   ],
   "source": [
    "# Tidak error jika kata tidak ada di training data\n",
    "\n",
    "kata = 'beckmans'\n",
    "try:\n",
    "    print(model_FT.wv.most_similar(kata))\n",
    "except:\n",
    "    print('error! kata \"',kata,'\" tidak ada di training data')\n",
    "# ini adalah kelebihan lain FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('began', 0.9998446702957153), ('julien', 0.9996268153190613), ('beamish', 0.9996228218078613), ('steven', 0.9996148347854614), ('soften', 0.9995564222335815), ('belasco', 0.9995546340942383), ('kitchen', 0.9995415210723877), ('given', 0.9994928240776062), ('buoys', 0.9994496703147888), ('lien', 0.9994242191314697)]\n"
     ]
    }
   ],
   "source": [
    "# Tapi hati-hati dengan hasil ini:\n",
    "print(model_FT.wv.most_similar('beckman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec error!\n",
      "[('classic', 0.9999449253082275), ('mid-february', 0.9999409914016724), ('mid-january', 0.9999346137046814), ('flows', 0.999931275844574), ('cardiothoracic', 0.9999278783798218), ('psychotic', 0.9999256134033203), ('multi-disciplinary', 0.9999158382415771), ('feburary', 0.999915599822998), ('apologize', 0.9999138116836548), ('marriage', 0.9999136924743652)]\n"
     ]
    }
   ],
   "source": [
    "# atau bahkan bisa seperti ini ...\n",
    "try:\n",
    "    print(model_wv.wv.most_similar('industri'))\n",
    "except:\n",
    "    print('Word2Vec error!')\n",
    "    \n",
    "try:\n",
    "    print(model_FT.wv.most_similar('industri'))\n",
    "except:\n",
    "    print('FastText error!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of Module\n",
    "\n",
    "<hr />\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
